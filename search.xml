<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hello World</title>
    <url>/2023/11/15/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very
first post. Check <a href="https://hexo.io/docs/">documentation</a> for
more info. If you get any problems when using Hexo, you can find the
answer in <a
href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or
you can ask me on <a
href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a
href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a
href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a
href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>EA-HAS-Bench</title>
    <url>/2023/11/16/EA-HAS-Bench/</url>
    <content><![CDATA[<h1 id="研究背景">1 研究背景</h1>
<p>随着深度学习技术的发展，训练数据和模型规模快速增长，导致训练模型的能耗也大幅增加，并对碳中和产生负面影响。</p>
<p>对于AutoML算法，例如<strong>神经架构搜索（Neural Architecture Search,
NAS）</strong>和<strong>超参数优化（Hyperparameter Optimization,
HPO）</strong>，避免了神经架构和超参数调整的人工处理，但需要重复训练大量计算密集型深度模型，从而导致显着的能源消耗和碳排放。</p>
<span id="more"></span>
<p>与此同时，现有的 NAS
研究主要集中于搜索神经网络模型的资源成本上，如参数大小、浮点运算（FLOPS）或延迟，而直接分析模型性能和能源消耗的研究则很少。</p>
<p>因此，必须开发具备能源感知 (Energy Aware, EA) 的 AutoML
方法，使其能够<strong>权衡模型性能和能量消耗</strong>。</p>
<h1 id="研究成果">2 研究成果</h1>
<ul>
<li>提出首个大规模能耗感知超参数和架构搜索基准——EA-HAS-Bench（energy-aware
hyperparameter and architecture search benchmark
），以在性能和搜索能耗之间实现更好的权衡；</li>
<li>EA-HAS-Bench
提供了一个大规模的架构/超参数联合搜索空间，涵盖了与能耗相关的多样化配置；</li>
<li>开发了一种新的代理模型，专门为大型联合搜索空间设计，该模型提出了一种基于B́ezier曲线的模型来预测具有无限形状和长度的学习曲线。</li>
</ul>
<h1 id="研究内容">3 研究内容</h1>
<h2 id="ea-has-bench-的创建">3.1 EA-HAS-Bench 的创建</h2>
<p>EA-HAS-Bench 的搜索空间由两部分组成：</p>
<ol type="1">
<li>网络架构空间-RegNet（包含<span class="math inline">\(6\times
10^{7}\)</span>个采样点）</li>
<li>用于优化和训练的超参数空间（包含<span
class="math inline">\(540\)</span>个采样点）</li>
</ol>
<img src="/2023/11/16/EA-HAS-Bench/img1.png" class="" title="img1">
<p>由于 EA-HAS-Bench 的搜索空间范围很大，所以即使对于 CIFAR10
这样的小数据集，直接训练空间中的所有配置也不可行。同时，一些如模型性能曲线、搜索能耗成本和运行时间等指标无法直接测量。而对于学习曲线预测，现有
NAS-Bench 提出的代理模型不适用于 EA-HAS-Bench，因为 EA-HAS-Bench
搜索空间中包含不同的最大训练轮次。需要提出一个适用于 EA-HAS-Bench
的全新代理模型。</p>
<p>研究创建了 <strong>B ́ezier Curved-based
Surrogate（BCS）模型</strong>，选择贝塞尔曲线来拟合任意长度的学习曲线。贝塞尔曲线的形状完全由其控制点决定，对任意长度的学习曲线进行回归的任务会被简化为预测贝塞尔曲线控制点。</p>
<p>BCS 模型由以下部分组成：</p>
<ul>
<li>超参数编码器</li>
<li>架构编码器</li>
<li>学习曲线预测网络</li>
</ul>
<img src="/2023/11/16/EA-HAS-Bench/img2.png" class="" title="img2">
<p>当给定一个 RegNet 架构和超参数配置后：</p>
<ol type="1">
<li>将超参数配置表示为独热向量，送入超参数编码器；</li>
<li>将网络配置参数归一化并拼接为向量，送入架构编码器；</li>
<li>将超参数表征和结构表征送入学习曲线预测器，以估计贝塞尔曲线的起始/终止点和控制点。其中学习曲线预测器也由两个分支构成：
<ol type="1">
<li>一个分支预测贝塞尔曲线起点和终点的纵坐标（因为横坐标已知，分别为 0
和 maximum epoch）；</li>
<li>另一分支预测贝塞尔曲线的其他控制点；</li>
</ol></li>
<li>输出通过贝塞尔曲线拟合的学习曲线。</li>
</ol>
<p>在评估代理模型时，使用的评价指标为确定系数 <span
class="math inline">\(R^2\)</span> 和 Kendall 秩相关系数 <span
class="math inline">\(\tau\)</span>，比较的对象为：</p>
<ul>
<li>六个基于参数化学习曲线的模型（exp3, ilog2, pow2, log power, log
linear, vapor pressure）</li>
<li>三个 NAS-Bench-X11 中的代理模型（SVD-XGB, SVD-LGB, SVD-MLP）</li>
</ul>
<img src="/2023/11/16/EA-HAS-Bench/img3.png" class="" title="img3">
<p>经过对比分析，基于参数化学习曲线的模型函数不能很好地拟合 EA-HAS-Bench
中的真实学习曲线，有些高阶函数甚至不能收敛，因为在代理模型中，不同参数的重要性差异很大。</p>
<p>对于 BCS 模型，在 CIFAR10 上，R2 为 0.787，KT 为 0.686，Pearson
相关性为 0.89。在 TinyImageNet 上，R2 为 0.959，KT 为 0.872，Pearson
相关性为 0.97。</p>
<h2 id="ea-nas-bench-的对比">3.2 EA-NAS-Bench 的对比</h2>
<p>与现有的 NAS
基准相比（NAS-Bench-101、NAS-Bench-201、NAS-Bench-301）EA-HAS-Bench
有三个明显的区别：</p>
<ol type="1">
<li>EA-HAS-Bench
在搜索空间的配置类型方面更加多样化，包含了模型架构和训练超参数。
<ul>
<li>与 NAS-Bench-101/201 相比，EA-HAS-Bench
中的配置覆盖了更大的性能范围</li>
<li>与 NAS-Bench-301 相比，虽然 NAS-Bench-301 使用的 DARTS
包含更多的架构，空间中模型的性能没有那么多样化</li>
</ul></li>
<li>EA-HAS-Bench 一个更现实的搜索空间，BSC
的提出能够预测不同最大训练次数的学习曲线。
<ul>
<li>NAS-Bench-X11
是现有的、唯一提供了整个训练过程的完整学习曲线的代用模型方法，但却只适用于具有固定最大训练的学习曲线</li>
</ul></li>
<li>EA-HAS-Bench 提供了一个全周期的能耗，包括训练和推理期间的能耗。
<ul>
<li>大多数现有的基准测试使用模型训练时间作为训练资源预算，不能准确地估计能源成本</li>
<li>能源成本不仅对应于训练时间，还与每单位时间的能量成本有关，而这受到架构和超参数配置的影响</li>
</ul></li>
</ol>
<h2 id="ea-nas-bench-的分析">3.3 EA-NAS-Bench 的分析</h2>
<p><strong>探究不同超参数如何具体影响搜索到的模型性能</strong>。</p>
<ul>
<li>使用经验累积分布empirical cumulative distribution
(ECDF)，来评估搜索空间的质量，从搜索空间中抽取了一组配置并描述其误差分布。</li>
<li>最终得到结论：学习率和优化器的选择可能对搜索空间质量有重要的影响。最大训练次数的大小也与搜索空间的质量呈正相关关系。</li>
</ul>
<p><strong>探究训练时间、训练能源消耗（Training Energy Consumption,
TEC）和精度之间的相关关系</strong>。</p>
<ul>
<li>TEC
和训练时间呈现一定的正相关关系，但相关性不强，尽管训练模型的时间越长，可能产生的能耗成本越高，但最终的成本仍然取决于如功率等许多其他因素。</li>
<li>与此同时，训练一个模型更长的时间（或用更多的能耗）并不能保证更好的准确性，而简单地寻找最佳精度可能也不符合成本效益，因为有相当多的配置具有相同的精度水平。</li>
</ul>
<h2 id="ea-has-bench-的使用">3.4 EA-HAS-Bench 的使用</h2>
<p>首先，评估了一系列算法在搜索能耗成本和模型性能之间的权衡，所评估的算法有：</p>
<ul>
<li>4种单保真算法：随机搜索（RS）、局部搜索（LS）、正则化进化（REA）和
BANANAS</li>
<li>2种多保真算法：Hyperband（HB）和贝叶斯优化Hyperband（BOHB）</li>
</ul>
<img src="/2023/11/16/EA-HAS-Bench/img4.jpg" class="" title="img4">
<p>对于单保真算法，LS是两个数据集中表现最好的算法。这表明，与 NAS-Bench
类似，具有联合搜索空间的 HAS-Bench 也具有局部性，这种属性使搜索空间中
"接近 "的点往往具有相似的性能。</p>
<p>接下来，用另一个与能量相关的目标改造了几种现有的 AutoML
算法，并验证了能量感知 AutoML 在 EA-HAS-Bench上的有效性。</p>
<img src="/2023/11/16/EA-HAS-Bench/img5.png" class="" title="img5">
]]></content>
      <categories>
        <category>Literature Notes</category>
      </categories>
      <tags>
        <tag>AutoML</tag>
      </tags>
  </entry>
</search>
